# Transformer Architecture

## Overview

This repository contains an implementation of the Transformer architecture from scratch, written in Python and PyTorch. 

The Transformer is a powerful neural network architecture that has been shown to achieve state-of-the-art performance on a wide range of natural language processing tasks, including language modeling, machine translation, and sentiment analysis.

## References
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper.
- [The Illustrated Transformer by Jay Alammar](http://jalammar.github.io/illustrated-transformer/) by Jay Alammar.
- [Pytorch Transformers from Scratch](https://www.youtube.com/watch?v=U0s0f995w14&t=729s) by Aladdin Persson.
- [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/) by Sasha Rush.
- [Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html) by Sebastian Raschka.
- [TRANSFORMERS FROM SCRATCH](https://peterbloem.nl/blog/transformers) by Peter Bloem.


